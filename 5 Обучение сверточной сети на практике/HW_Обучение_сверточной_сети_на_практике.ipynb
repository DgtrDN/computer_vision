{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O3ixeUiUcal_",
        "outputId": "3984e828-ff55-44ed-ae10-19dd11736fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Распределение классов в train: [12500 12500]\n",
            "Распределение классов в val: [12500]\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 11.2 GiB for an array with shape (20000, 224, 224, 3) and data type float32",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 130\u001b[0m\n\u001b[0;32m    120\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    121\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    122\u001b[0m     ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m     TensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir)\n\u001b[0;32m    126\u001b[0m ]\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Обучение модели (уменьшено до 30 эпох)\u001b[39;00m\n\u001b[0;32m    129\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m--> 130\u001b[0m     train_datagen\u001b[38;5;241m.\u001b[39mflow(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    131\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# Уменьшено с 100 до 30\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_datagen\u001b[38;5;241m.\u001b[39mflow(X_val, y_val, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m),\n\u001b[0;32m    133\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    134\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mclass_weights,\n\u001b[0;32m    135\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Визуализация\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_metrics\u001b[39m(history):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1103\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow\u001b[39m(\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1091\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1102\u001b[0m ):\n\u001b[1;32m-> 1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NumpyArrayIterator(\n\u001b[0;32m   1104\u001b[0m         x,\n\u001b[0;32m   1105\u001b[0m         y,\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1107\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1108\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1109\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1110\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1111\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[0;32m   1112\u001b[0m         save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[0;32m   1113\u001b[0m         save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[0;32m   1114\u001b[0m         save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[0;32m   1115\u001b[0m         ignore_class_split\u001b[38;5;241m=\u001b[39mignore_class_split,\n\u001b[0;32m   1116\u001b[0m         subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[0;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:609\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    607\u001b[0m             y \u001b[38;5;241m=\u001b[39m y[split_idx:]\n\u001b[1;32m--> 609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_misc \u001b[38;5;241m=\u001b[39m x_misc\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.2 GiB for an array with shape (20000, 224, 224, 3) and data type float32"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import log_loss, classification_report, confusion_matrix, roc_curve, auc\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Кастомный callback для записи learning rate\n",
        "class LRTracker(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
        "\n",
        "# Пути к данным\n",
        "train_dir = \"data/train/train\"\n",
        "val_dir = \"data/test/test\"\n",
        "\n",
        "# Загрузка и проверка данных\n",
        "def load_and_validate_data():\n",
        "    train_images = [os.path.join(train_dir, img) for img in os.listdir(train_dir) if img.endswith(('.jpg', '.png'))]\n",
        "    val_images = [os.path.join(val_dir, img) for img in os.listdir(val_dir) if img.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    assert len(train_images) > 0, \"Нет изображений в train\"\n",
        "    assert len(val_images) > 0, \"Нет изображений в test\"\n",
        "\n",
        "    return train_images, val_images\n",
        "\n",
        "# Улучшенная функция загрузки изображений\n",
        "def load_images(image_paths):\n",
        "    images, labels = [], []\n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            img = cv2.resize(img, (224, 224))  # EfficientNet стандартный размер\n",
        "            img = preprocess_input(img)  # Специфичная предобработка\n",
        "            images.append(img)\n",
        "            labels.append(1 if \"dog\" in os.path.basename(img_path).lower() else 0)\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка загрузки {img_path}: {str(e)}\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Загрузка данных\n",
        "train_images, val_images = load_and_validate_data()\n",
        "X_train, y_train = load_images(train_images)\n",
        "X_val, y_val = load_images(val_images)\n",
        "\n",
        "# Проверка баланса классов\n",
        "print(f\"\\nРаспределение классов в train: {np.bincount(y_train)}\")\n",
        "print(f\"Распределение классов в val: {np.bincount(y_val)}\")\n",
        "\n",
        "# Разделение данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# Улучшенная аугментация с препроцессингом\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.85, 1.15],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator()\n",
        "\n",
        "# Веса классов для борьбы с дисбалансом\n",
        "class_weights = {0: len(y_train)/(2*np.bincount(y_train)[0]),\n",
        "                 1: len(y_train)/(2*np.bincount(y_train)[1])}\n",
        "\n",
        "# Создание модели\n",
        "def create_model():\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Замораживаем только первые 3 слоя\n",
        "    for layer in base_model.layers[:3]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(1024, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    optimizer = AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                 tf.keras.metrics.Precision(name='precision'),\n",
        "                 tf.keras.metrics.Recall(name='recall'),\n",
        "                 tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "# Callbacks\n",
        "log_dir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_auc', patience=5, mode='max', verbose=1, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', monitor='val_auc', save_best_only=True, mode='max'),\n",
        "    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3, min_lr=1e-6, mode='max', verbose=1),\n",
        "    LRTracker(),\n",
        "    TensorBoard(log_dir=log_dir)\n",
        "]\n",
        "\n",
        "# Обучение модели (уменьшено до 30 эпох)\n",
        "history = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=32, shuffle=True),\n",
        "    epochs=30,  # Уменьшено с 100 до 30\n",
        "    validation_data=val_datagen.flow(X_val, y_val, batch_size=32),\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Визуализация\n",
        "def plot_metrics(history):\n",
        "    plt.figure(figsize=(18, 12))\n",
        "\n",
        "    metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        plt.plot(history.history[metric], label=f'Train {metric}')\n",
        "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel(metric)\n",
        "        plt.legend()\n",
        "\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.plot(history.history['lr'], label='Learning Rate')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('LR')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(history)\n",
        "\n",
        "# Оценка модели с добавлением LogLoss\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    test_generator = val_datagen.flow(X_test, y_test, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Предсказания с оптимальным порогом\n",
        "    y_pred = model.predict(test_generator)\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "    print(f\"\\nОптимальный порог: {optimal_threshold:.4f}\")\n",
        "    y_pred_classes = (y_pred > optimal_threshold).astype(int)\n",
        "\n",
        "    # Вычисление LogLoss\n",
        "    test_loss = log_loss(y_test, y_pred)\n",
        "    print(f\"\\nLogLoss на тестовых данных: {test_loss:.4f}\")\n",
        "\n",
        "    # Метрики\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_classes, target_names=['Cat', 'Dog']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Cat', 'Dog'],\n",
        "                yticklabels=['Cat', 'Dog'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "evaluate_model(model, X_test, y_test)\n",
        "\n",
        "# Функция предсказания с оптимальным порогом\n",
        "def predict_image(img_path, model, threshold=None):\n",
        "    try:\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Изображение {img_path} не загружено\")\n",
        "\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        img = preprocess_input(img)\n",
        "        img_array = np.expand_dims(img, axis=0)\n",
        "\n",
        "        # Автоматическое определение порога\n",
        "        if threshold is None:\n",
        "            test_generator = val_datagen.flow(X_test, y_test, batch_size=32, shuffle=False)\n",
        "            y_pred = model.predict(test_generator)\n",
        "            fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "            threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "\n",
        "        probability = model.predict(img_array, verbose=0)[0][0]\n",
        "        class_name = 'Dog' if probability > threshold else 'Cat'\n",
        "\n",
        "        return {\n",
        "            'class': class_name,\n",
        "            'probability': float(probability),\n",
        "            'threshold': float(threshold),\n",
        "            'confidence': float(probability if class_name == 'Dog' else 1 - probability)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при предсказании: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Пример использования\n",
        "sample_image = val_images[0] if val_images else train_images[0]\n",
        "prediction = predict_image(sample_image, model)\n",
        "if prediction:\n",
        "    print(\"\\nПример предсказания:\")\n",
        "    print(f\"Изображение: {os.path.basename(sample_image)}\")\n",
        "    print(f\"Класс: {prediction['class']}\")\n",
        "    print(f\"Вероятность: {prediction['probability']:.4f}\")\n",
        "    print(f\"Порог: {prediction['threshold']:.4f}\")\n",
        "    print(f\"Уверенность: {prediction['confidence']:.2%}\")\n",
        "\n",
        "# Сохранение модели\n",
        "model.save('cats_dogs_classifier.h5')\n",
        "print(\"\\nМодель сохранена как cats_dogs_classifier.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP9tlor7VpUh",
        "outputId": "cfbe8545-01e9-492f-d778-2fbf69d4bca2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Делаем предсказания на тестовом наборе данных\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Преобразуем предсказания в бинарные метки (0 - кошка, 1 - собака)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m test_pred_labels \u001b[38;5;241m=\u001b[39m (test_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Делаем предсказания на тестовом наборе данных\n",
        "test_pred = model.predict(X_test)\n",
        "\n",
        "# Преобразуем предсказания в бинарные метки (0 - кошка, 1 - собака)\n",
        "# test_pred_labels = (test_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Создаем DataFrame для submission\n",
        "submission_df = pd.DataFrame({\n",
        "    # 'ImageId': range(1, len(test_pred_labels) + 1),\n",
        "    # 'Label': test_pred_labels\n",
        "    'id': range(1, len(test_pred) + 1),\n",
        "    'label': test_pred\n",
        "})\n",
        "\n",
        "# Сохраняем CSV\n",
        "submission_path = 'submission.csv'\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"Файл сохранен: {submission_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "If class_mode=\"binary\" there must be 2 classes. Found 1 classes.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Создаем генераторы\u001b[39;00m\n\u001b[0;32m    104\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m create_generator(train_images, train_labels, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m val_generator \u001b[38;5;241m=\u001b[39m create_generator(val_images, val_labels, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Проверка классов\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИндексы классов:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_generator\u001b[38;5;241m.\u001b[39mclass_indices)\n",
            "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mcreate_generator\u001b[1;34m(image_paths, labels, batch_size, target_size, shuffle)\u001b[0m\n\u001b[0;32m     54\u001b[0m str_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Создаем генератор из массива\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m generator \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[0;32m     58\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: image_paths, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: str_labels}),\n\u001b[0;32m     59\u001b[0m     x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     60\u001b[0m     y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     61\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m     62\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     63\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     64\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generator\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1208\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   1202\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates is deprecated, you can drop duplicates \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby using the pandas.DataFrame.drop_duplicates method.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1205\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m   1206\u001b[0m     )\n\u001b[1;32m-> 1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameIterator(\n\u001b[0;32m   1209\u001b[0m     dataframe,\n\u001b[0;32m   1210\u001b[0m     directory,\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1212\u001b[0m     x_col\u001b[38;5;241m=\u001b[39mx_col,\n\u001b[0;32m   1213\u001b[0m     y_col\u001b[38;5;241m=\u001b[39my_col,\n\u001b[0;32m   1214\u001b[0m     weight_col\u001b[38;5;241m=\u001b[39mweight_col,\n\u001b[0;32m   1215\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m   1216\u001b[0m     color_mode\u001b[38;5;241m=\u001b[39mcolor_mode,\n\u001b[0;32m   1217\u001b[0m     classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m   1218\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[0;32m   1219\u001b[0m     data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[0;32m   1220\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1221\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1222\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1223\u001b[0m     save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[0;32m   1224\u001b[0m     save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[0;32m   1225\u001b[0m     save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[0;32m   1226\u001b[0m     subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[0;32m   1227\u001b[0m     interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   1228\u001b[0m     validate_filenames\u001b[38;5;241m=\u001b[39mvalidate_filenames,\n\u001b[0;32m   1229\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1230\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:751\u001b[0m, in \u001b[0;36mDataFrameIterator.__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# check that inputs match the required class_mode\u001b[39;00m\n\u001b[1;32m--> 751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(df, x_col, y_col, weight_col, classes)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    753\u001b[0m     validate_filenames\n\u001b[0;32m    754\u001b[0m ):  \u001b[38;5;66;03m# check which image files are valid and keep them\u001b[39;00m\n\u001b[0;32m    755\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_valid_filepaths(df, x_col)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:833\u001b[0m, in \u001b[0;36mDataFrameIterator._check_params\u001b[1;34m(self, df, x_col, y_col, weight_col, classes)\u001b[0m\n\u001b[0;32m    828\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    829\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf class_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m there must be 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    830\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses. \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m class/es were given.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(classes))\n\u001b[0;32m    831\u001b[0m             )\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m df[y_col]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 833\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    834\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf class_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m there must be 2 classes. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    835\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m classes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(df[y_col]\u001b[38;5;241m.\u001b[39mnunique())\n\u001b[0;32m    836\u001b[0m         )\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# check values are string, list or tuple if class_mode is categorical\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mValueError\u001b[0m: If class_mode=\"binary\" there must be 2 classes. Found 1 classes."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import log_loss, classification_report, confusion_matrix, roc_curve, auc\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Кастомный callback для записи learning rate\n",
        "class LRTracker(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
        "\n",
        "# Пути к данным\n",
        "train_dir = \"data/train/train\"\n",
        "val_dir = \"data/test/test\"\n",
        "\n",
        "# Загрузка и проверка данных\n",
        "def load_and_validate_data():\n",
        "    train_images = [os.path.join(train_dir, img) for img in os.listdir(train_dir) if img.endswith(('.jpg', '.png'))]\n",
        "    val_images = [os.path.join(val_dir, img) for img in os.listdir(val_dir) if img.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    assert len(train_images) > 0, \"Нет изображений в train\"\n",
        "    assert len(val_images) > 0, \"Нет изображений в test\"\n",
        "\n",
        "    return train_images, val_images\n",
        "\n",
        "# Создание генератора данных\n",
        "def create_generator(image_paths, labels, batch_size=16, target_size=(224, 224), shuffle=True):\n",
        "    datagen = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "        rotation_range=25,\n",
        "        width_shift_range=0.15,\n",
        "        height_shift_range=0.15,\n",
        "        shear_range=0.15,\n",
        "        zoom_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=[0.85, 1.15],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    \n",
        "    # Конвертируем числовые метки в строковые\n",
        "    str_labels = ['dog' if label == 1 else 'cat' for label in labels]\n",
        "    \n",
        "    # Создаем генератор из массива\n",
        "    generator = datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': image_paths, 'class': str_labels}),\n",
        "        x_col='filename',\n",
        "        y_col='class',\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "    return generator\n",
        "\n",
        "# Создание модели\n",
        "def create_model():\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    for layer in base_model.layers[:3]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(1024, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    optimizer = AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                 tf.keras.metrics.Precision(name='precision'),\n",
        "                 tf.keras.metrics.Recall(name='recall'),\n",
        "                 tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Основной код\n",
        "train_images, val_images = load_and_validate_data()\n",
        "train_labels = [1 if \"dog\" in os.path.basename(img).lower() else 0 for img in train_images]\n",
        "val_labels = [1 if \"dog\" in os.path.basename(img).lower() else 0 for img in val_images]\n",
        "\n",
        "# Создаем генераторы\n",
        "train_generator = create_generator(train_images, train_labels, batch_size=16)\n",
        "val_generator = create_generator(val_images, val_labels, batch_size=16, shuffle=False)\n",
        "\n",
        "# Проверка классов\n",
        "print(\"Индексы классов:\", train_generator.class_indices)\n",
        "print(\"Распределение классов в train:\", np.bincount(train_generator.classes))\n",
        "print(\"Распределение классов в val:\", np.bincount(val_generator.classes))\n",
        "\n",
        "# Веса классов для балансировки\n",
        "class_counts = np.bincount(train_generator.classes)\n",
        "total_samples = len(train_generator.classes)\n",
        "class_weights = {\n",
        "    0: total_samples / (2 * class_counts[0]),  # Вес для класса 'cat'\n",
        "    1: total_samples / (2 * class_counts[1])   # Вес для класса 'dog'\n",
        "}\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "# Callbacks\n",
        "log_dir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_auc', patience=5, mode='max', verbose=1, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', monitor='val_auc', save_best_only=True, mode='max'),\n",
        "    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3, min_lr=1e-6, mode='max', verbose=1),\n",
        "    LRTracker(),\n",
        "    TensorBoard(log_dir=log_dir)\n",
        "]\n",
        "\n",
        "# Обучение модели\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=30,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=len(val_generator),\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Визуализация метрик\n",
        "def plot_metrics(history):\n",
        "    plt.figure(figsize=(18, 12))\n",
        "\n",
        "    metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        plt.plot(history.history[metric], label=f'Train {metric}')\n",
        "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel(metric)\n",
        "        plt.legend()\n",
        "\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.plot(history.history['lr'], label='Learning Rate')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('LR')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(history)\n",
        "\n",
        "# Оценка модели\n",
        "def evaluate_model(model, generator):\n",
        "    # Получаем истинные метки\n",
        "    y_true = generator.classes\n",
        "    \n",
        "    # Делаем предсказания\n",
        "    generator.reset()\n",
        "    y_pred = model.predict(generator, steps=len(generator))\n",
        "    \n",
        "    # Находим оптимальный порог\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "    y_pred_classes = (y_pred > optimal_threshold).astype(int)\n",
        "\n",
        "    print(f\"\\nОптимальный порог: {optimal_threshold:.4f}\")\n",
        "    \n",
        "    # Вычисляем метрики\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred_classes, target_names=['Cat', 'Dog']))\n",
        "    \n",
        "    print(f\"\\nLogLoss: {log_loss(y_true, y_pred):.4f}\")\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred_classes)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Cat', 'Dog'],\n",
        "                yticklabels=['Cat', 'Dog'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "    \n",
        "    # ROC Curve\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nОценка на валидационных данных:\")\n",
        "evaluate_model(model, val_generator)\n",
        "\n",
        "# Сохранение модели\n",
        "model.save('cats_dogs_classifier.h5')\n",
        "print(\"\\nМодель сохранена как cats_dogs_classifier.h5\")\n",
        "\n",
        "# Функция для предсказания на новых изображениях\n",
        "def predict_image(img_path, model, threshold=None):\n",
        "    try:\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Не удалось загрузить изображение: {img_path}\")\n",
        "            \n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        img = preprocess_input(img)\n",
        "        img_array = np.expand_dims(img, axis=0)\n",
        "        \n",
        "        probability = model.predict(img_array, verbose=0)[0][0]\n",
        "        \n",
        "        if threshold is None:\n",
        "            threshold = 0.5  # Значение по умолчанию\n",
        "            \n",
        "        class_name = 'Dog' if probability > threshold else 'Cat'\n",
        "        confidence = probability if class_name == 'Dog' else 1 - probability\n",
        "        \n",
        "        return {\n",
        "            'class': class_name,\n",
        "            'probability': float(probability),\n",
        "            'confidence': float(confidence),\n",
        "            'threshold': float(threshold)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при предсказании: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Пример предсказания\n",
        "if val_images:\n",
        "    sample_image = val_images[0]\n",
        "    prediction = predict_image(sample_image, model)\n",
        "    if prediction:\n",
        "        print(\"\\nПример предсказания:\")\n",
        "        print(f\"Изображение: {os.path.basename(sample_image)}\")\n",
        "        print(f\"Класс: {prediction['class']}\")\n",
        "        print(f\"Вероятность: {prediction['probability']:.4f}\")\n",
        "        print(f\"Уверенность: {prediction['confidence']:.2%}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
